{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMHf6MeT4QfX+vquGXoyTfX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VFvTMCCaEU7_","executionInfo":{"status":"ok","timestamp":1663068040075,"user_tz":-540,"elapsed":41645,"user":{"displayName":"yi liu","userId":"07321536629726095967"}},"outputId":"50b7d965-afee-4e52-a669-67ab156cff32"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Data manipulation\n","import pandas as pd\n","\n","# Visualization\n","import plotly.express as px\n","\n","# Sklearn\n","from sklearn.model_selection import train_test_split # for splitting data into train and test samples\n","from sklearn.svm import SVC # for Support Vector Classification baseline model\n","from sklearn.semi_supervised import SelfTrainingClassifier # for Semi-Supervised learning\n","from sklearn.metrics import classification_report # for model evaluation metrics\n","from sklearn.metrics import f1_score\n","\n","from sklearn.utils import shuffle\n","import math\n","import os\n","import numpy as np\n","\n","from keras.layers import Input, Dense\n","from keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","import keras.backend as K\n","\n","import time\n","import datetime\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# Read in data\n","FOLDER = \"/content/drive/MyDrive/Emotion-RST/marketing_campaign.csv\"\n","df = pd.read_csv(FOLDER, \n","                 encoding='utf-8', delimiter=';',\n","                 usecols=['ID', 'Year_Birth', 'Marital_Status', 'Income', 'Kidhome', 'Teenhome', 'MntWines', 'MntMeatProducts'])\n","# Create a flag to denote whether the person has any dependants at home (either kids or teens)\n","df['Dependents_Flag']=df.apply(lambda x: 1 if x['Kidhome']+x['Teenhome']>0 else 0, axis=1)\n","\n","# Print datafram"],"metadata":{"id":"myHv40oAEtVV","executionInfo":{"status":"ok","timestamp":1663068045795,"user_tz":-540,"elapsed":670,"user":{"displayName":"yi liu","userId":"07321536629726095967"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["#将训练集划分为子训练集、验证集，比例是0.75:0.25\n","df_train, df_test = train_test_split(df, test_size=0.25, random_state=0)\n","print('Size of train dataframe: ', df_train.shape[0])\n","print('Size of test dataframe: ', df_test.shape[0])\n","print(\"#\"*50)\n","\n","#现在让我们屏蔽训练数据中 95% 的标签，并创建一个使用“-1”表示未标记（屏蔽）数据的目标变量：\n","# Create a flag for label masking\n","df_train['Random_Mask'] = True\n","df_train.loc[df_train.sample(frac=0.05, random_state=0).index, 'Random_Mask'] = False\n","# Create a new target colum with labels. The 1's and 0's are original labels and -1 represents unlabeled (masked) data\n","df_train['Dependents_Target']=df_train.apply(lambda x: x['Dependents_Flag'] if x['Random_Mask']==False else -1, axis=1)\n","# Show target value distribution\n","print('Target Value Distribution:')\n","print(df_train['Dependents_Target'].value_counts())\n","\n","#创建Ds,Du,Ds_dev\n","Ds = df_train[df_train['Dependents_Target']!=-1] #labeled training data in masked Dataset\n","Du = df_train[df_train['Dependents_Target']==-1] #unlabeled training data in masked Dataset\n","Ds_dev = df_test\n","\n","print(\"#\"*50)\n","print('Size of labeled seen data Ds dataframe: ', Ds.shape[0])\n","print('Size of unlabeled unseen data Du dataframe: ', Du.shape[0])\n","print('Size of seen validation set Ds_dev dataframe: ', Ds_dev.shape[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fyqLhKVkExJZ","executionInfo":{"status":"ok","timestamp":1663068049158,"user_tz":-540,"elapsed":269,"user":{"displayName":"yi liu","userId":"07321536629726095967"}},"outputId":"0086b116-ccb5-4423-d6af-29cd6d893ba8"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Size of train dataframe:  1680\n","Size of test dataframe:  560\n","##################################################\n","Target Value Distribution:\n","-1    1596\n"," 1      58\n"," 0      26\n","Name: Dependents_Target, dtype: int64\n","##################################################\n","Size of labeled seen data Ds dataframe:  84\n","Size of unlabeled unseen data Du dataframe:  1596\n","Size of seen validation set Ds_dev dataframe:  560\n"]}]},{"cell_type":"code","source":["def build_model():\n","    \"\"\"basic model.\n","    \"\"\"\n","    inputs = Input(shape=(4,), name='ob_input')\n","    x = Dense(16, activation='relu')(inputs)\n","    x = Dense(16, activation='relu')(x)\n","    x = Dense(1, activation='sigmoid')(x)\n","    model = Model(inputs=inputs, outputs=x)\n","    return model\n","\n","def calloss(y_true, y_pred):\n","      \"\"\"loss function.\n","      Arguments:\n","          y_true: (action, reward)\n","          y_pred: action_prob\n","\n","      Returns:\n","          loss: reward loss\n","      \"\"\"\n","      action_pred = y_pred\n","      action_true, discount_episode_reward = y_true[:, 0], y_true[:, 1]\n","\n","      action_true = K.reshape(action_true, (-1, 1))\n","      loss = K.binary_crossentropy(action_true, action_pred)\n","      loss = loss * K.flatten(discount_episode_reward)\n","\n","      return loss\n","\n","gamma = 0.75    \n","def discount_reward(rewards):\n","      \"\"\"Discount reward\n","      Arguments:\n","          rewards: rewards in a episode.\n","      \"\"\"\n","      # compute the discounted reward backwards through time.\n","      discount_rewards = np.zeros_like(rewards, dtype=np.float32)\n","      cumulative = 0.\n","      for i in reversed(range(len(rewards))):\n","          cumulative = cumulative * gamma + rewards[i]\n","          discount_rewards[i] = cumulative\n","\n","      # size the rewards to be unit normal (helps control the gradient estimator variance).\n","      discount_rewards -= np.mean(discount_rewards)\n","      discount_rewards //= np.std(discount_rewards)\n","\n","      discount_rewards = discount_rewards+1\n","      return list(discount_rewards)\n","\n","def getnewBpk(Bk,probs):\n","  Bpk = Bk.copy()\n","  actions = []\n","  for i in range(len(probs)):\n","    prob = probs[i][0]\n","    action = np.random.choice(np.array(range(2)), size=1, p=[1 - prob, prob])[0]\n","    actions.append(action)\n","  Bpk['action'] = actions\n","  Bpk = Bpk[Bpk['action']!=0]\n","  return Bpk,actions"],"metadata":{"id":"Y8fEfk14MayA","executionInfo":{"status":"ok","timestamp":1663068052149,"user_tz":-540,"elapsed":3,"user":{"displayName":"yi liu","userId":"07321536629726095967"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fxy3IVA6Mg5f","executionInfo":{"status":"ok","timestamp":1663068056688,"user_tz":-540,"elapsed":373,"user":{"displayName":"yi liu","userId":"07321536629726095967"}},"outputId":"75ae8c19-b9f0-4ba6-90f8-faa917fcc375"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(Adam, self).__init__(name, **kwargs)\n"]}]},{"cell_type":"code","source":["svc_model = SVC(kernel='rbf', \n","    probability=True, \n","    C=1.0, # default = 1.0\n","    gamma='scale', # default = 'scale'\n","    random_state=0\n",")\n","\n","policy_model = build_model()\n","policy_model.compile(loss=calloss, optimizer=Adam(lr=0.01))\n","\n","class DataSelectController:\n","    def __init__(self,df_train,df_test):\n","        self.df_train = df_train #\n","        self.df_test = df_test #\n","\n","        self.labeled_train_dataset = df_train[df_train['Dependents_Target']!=-1] #\n","        self.unlabeled_train_dataset = df_train[df_train['Dependents_Target']==-1] #unlabeled training data in masked Dataset\n","        \n","        self.validation_dataset = df_test#\n","        self.unlabeled_validation_dataset = pd.DataFrame(columns=df_test.columns)\n","\n","        self.pseudo_labeled_dataset = pd.DataFrame(columns=self.labeled_train_dataset.columns)\n","        #self.pseudo_validation_dataset = pd.DataFrame(columns=self.labeled_train_dataset.columns)\n","\n","        self.base_svc_model = self.train_svc_model()\n","        self.policy_model = build_model()\n","        self.policy_model.compile(loss=calloss, optimizer=Adam(lr=0.01))\n","        pass\n","\n","    #inital environment\n","    def reset(self):\n","        self.labeled_train_dataset = self.df_train[df_train['Dependents_Target']!=-1] #\n","        self.unlabeled_train_dataset = self.df_train[df_train['Dependents_Target']==-1] #unlabeled training data in masked Dataset\n","        \n","        self.validation_dataset = self.df_test\n","        self.unlabeled_validation_dataset = pd.DataFrame(columns=self.df_test.columns)\n","\n","        self.pseudo_labeled_dataset = pd.DataFrame(columns=self.labeled_train_dataset.columns)\n","        #self.pseudo_validation_dataset = pd.DataFrame(columns=self.labeled_train_dataset.columns)\n","        self.base_svc_model = self.train_svc_model()\n","        pass\n","\n","    #update environment,每一个episode，更新一次pseudo_labeled_dataset，pseudo_validation_dataset，unlabeled_train_dataset\n","    def render(self):                     \n","        pass\n","\n","    #Train svc_model using trainset\n","    def train_svc_model_xy(self,trainset_x,trainset_y):\n","        clf = svc_model.fit(trainset_x,trainset_y)\n","        self.base_svc_model = clf\n","        return clf\n","    \n","    def train_svc_model(self):\n","        D_train_ = self.labeled_train_dataset if self.pseudo_labeled_dataset.empty else pd.concat([self.labeled_train_dataset, self.pseudo_labeled_dataset])\n","        clf = svc_model.fit(D_train_[['MntMeatProducts', 'MntWines']], D_train_['Dependents_Flag'].values)\n","        self.base_svc_model = clf\n","        return clf\n","\n","    def predict_unlabeled_set(self, unlabeled_set):\n","        proba = self.base_svc_model.predict_proba(unlabeled_set[['MntMeatProducts', 'MntWines']])\n","        predicted_label = self.base_svc_model.predict(unlabeled_set[['MntMeatProducts', 'MntWines']])\n","\n","        U = unlabeled_set.copy()\n","        U['proba_A'] = [a[0] for a in proba]\n","        U['proba_B'] = [b[1] for b in proba]\n","        U['predicted_label'] = predicted_label\n","\n","        U_predicted = U\n","        return U_predicted\n","\n","    def rank_U(self,U_predicted):\n","        U_predicted['proba_max'] = U_predicted.loc[:,['proba_A','proba_B']].T.max()\n","        U_ranked = U_predicted.sort_values('proba_max',ascending = False)\n","        U_ranked = U_ranked.drop(columns=['proba_max'])\n","        return U_ranked\n","\n","\n","    def select_Batch_k_PolicyNetwrok(self,policy_model,Batch_k):\n","        Batch_k_judged = Batch_k.copy()\n","        data_ = Batch_k[['MntMeatProducts','MntWines','proba_A','proba_B']]\n","        data_ = (data_-data_.min())/(data_.max()-data_.min())\n","        confidences = policy_model.predict(data_)\n","        print(confidences.T)\n","        #input()\n","        actions = []\n","        for i in range(len(confidences)):\n","            prob = confidences[i][0]\n","            #if(prob==None):\n","            action = np.random.choice(np.array(range(2)), size=1, p=[1 - prob, prob])[0]\n","            actions.append(action)\n","        Batch_k_judged['action'] = actions\n","        Batch_k_selected = Batch_k_judged.copy()\n","        Batch_k_selected = Batch_k_selected[Batch_k_selected['action']!=0]\n","        Batch_k_unselected = Batch_k_judged.copy()\n","        Batch_k_unselected = Batch_k_unselected[Batch_k_unselected['action']==0]\n","        return Batch_k_judged, Batch_k_selected, Batch_k_unselected, actions\n","\n","    def cal_f1(self,clf_,validation_set):\n","\n","        y_pred = clf_.predict(validation_set[['MntMeatProducts', 'MntWines']])\n","        #validation_score_list.append(clf_.score(validation_dataset[['MntMeatProducts', 'MntWines']], validation_dataset['Dependents_Flag'].values))\n","        validation_set_f1 = f1_score(validation_set['Dependents_Flag'].values, y_pred, average='macro')##TODO\n","\n","        return validation_set_f1\n","        pass\n","\n","    #对当前状态的Unlabeled_set按照Batch分组进行Policy Network评估，再合并计算各自的Reward\n","    def step(self,U, batch):\n","        observation = None\n","        reward=0\n","        done = False\n","        info = \"\"\n","\n","        U = shuffle(U)#打乱U\n","        every_epoch_num = math.floor((len(U)/batch))\n","\n","        Batch_k_list = []\n","        Batch_actions_list=[]\n","        Batch_k_judged_list = []\n","        validation_f1_list= []\n","        pseudo_f1_list = []\n","        validation_score_list=[]\n","        pseudo_score_list=[]\n","        for k in range(batch):\n","            #get a batch Bk from U\n","            if k < (batch-1):\n","                Batch_k = U[every_epoch_num * k: every_epoch_num * (k + 1)]\n","            else:\n","                Batch_k = U[every_epoch_num * k:]\n","            \n","            Batch_k_judged, Batch_k_selected, Batch_k_unselected, actions = self.select_Batch_k_PolicyNetwrok(policy_model, Batch_k)\n","            Batch_k_list.append(Batch_k)\n","            Batch_actions_list.append(actions)\n","            Batch_k_judged_list.append(Batch_k_judged)\n","      \n","            #Train model clf' with Bpk\n","            try:\n","              clf_ = svc_model.fit(Batch_k_selected[['MntMeatProducts', 'MntWines']], Batch_k_selected['predicted_label'].values)\n","            except ValueError as e:\n","              print ('error type: ', type (e))\n","              print(Batch_k_selected['predicted_label'].values)\n","              done = True\n","              return observation, reward, done, info\n","\n","            validation_f1 = self.cal_f1(clf_,self.validation_dataset)\n","            validation_score_list.append(clf_.score(self.validation_dataset[['MntMeatProducts', 'MntWines']], self.validation_dataset['Dependents_Flag'].values))\n","            validation_f1_list.append(validation_f1)\n","\n","            if(self.pseudo_labeled_dataset.empty!=True):\n","                pseudo_f1 = self.cal_f1(clf_,self.pseudo_labeled_dataset)\n","                pseudo_f1_list.append(pseudo_f1)\n","                pseudo_score_list.append(clf_.score(self.pseudo_labeled_dataset[['MntMeatProducts', 'MntWines']], self.pseudo_labeled_dataset['Dependents_Flag'].values))\n","\n","        validation_f1_mean = np.mean(validation_f1_list)\n","        validation_f1_std = np.std(validation_f1_list)\n","\n","        pseudo_f1_mean = np.mean(pseudo_f1_list)\n","        pseudo_f1_std = np.std(pseudo_f1_list)\n","         \n","        for k in range(batch):\n","            reward1 = (validation_f1_list[k] - validation_f1_mean)/validation_f1_std\n","            reward2 = 0\n","            lamda =0\n","            if(self.pseudo_labeled_dataset.empty!=True):\n","              reward2 = (pseudo_f1_list[k] - pseudo_f1_mean)/pseudo_f1_std\n","              lamda =len(self.validation_dataset)/len(self.pseudo_labeled_dataset)\n","\n","            #reward = reward1 + lamda*reward2\n","            reward = reward1\n","            \n","            Batch_k = Batch_k_list[k]\n","            Batch_actions = Batch_actions_list[k]\n","            Batch_rewards = [reward]*len(Batch_actions)\n","            X = Batch_k[['MntMeatProducts', 'MntWines','proba_A','proba_B']]\n","            X = (X-X.min())/(X.max()-X.min())\n","            #print(\"Batch_actions\")\n","            print(\"  \",Batch_actions)\n","            #print(Batch_k['Dependents_Flag'].values)\n","            y = np.array(list(zip(Batch_actions, Batch_rewards)))\n","\n","            loss = policy_model.train_on_batch(X, y)\n","\n","            validation_score =validation_score_list[k]\n","            pseudo_score=0\n","            if(self.pseudo_labeled_dataset.empty!=True):\n","              pseudo_score = pseudo_score_list[k]\n","            print('   Batch: {} |Batch reward: {} |loss: {:.3f} |validation_score: {:.3f} |clf_pseudo_score: {:.3f}'.format( k,reward,loss,validation_score,pseudo_score))\n","\n","        #完成后，需要对各个临时数据存储进行更新\n","          #集合全部Batch_k_judged，根据'action'字段选出selected和unselected\n","          #将selected添加到self.pseudo_labeled_dataset\n","          #将selected从self.unlabeled_train_dataset中去除\n","        U_judged = pd.concat(Batch_k_judged_list)#集合全部Batch_k_judged，\n","        U_selected = U_judged[U_judged['action']==1]#根据'action'字段选出selected和unselected\n","        U_unselected = U_judged[U_judged['action']!=1]#根据'action'字段选出selected和unselected\n","\n","\n","        U_selected = U_selected.drop(['proba_A', 'proba_B','action'], axis=1)\n","        self.pseudo_labeled_dataset = U_selected if self.pseudo_labeled_dataset.empty else pd.concat([self.pseudo_labeled_dataset, U_selected])#dataframe 参数不同，有bug，TODO        \n","        \n","        U_selected = U_selected.drop(['predicted_label'], axis=1)\n","        self.unlabeled_train_dataset = pd.concat([self.unlabeled_train_dataset, U_selected, U_selected]).drop_duplicates(keep=False)\n","        \n","        \n","        #print(self.unlabeled_train_dataset.columns)\n","        #print(U_selected.columns)\n","        print('U_selected:{}| pseudo_labeled_dataset:{}| unlabeled_train_dataset:{}|'.format(len(U_selected),len(self.pseudo_labeled_dataset),len(self.unlabeled_train_dataset)))\n","        #input()\n","\n","        return observation, reward, done, info\n","\n","\n","iteration = 1000\n","episode = 10 #5000\n","batch = 7\n","Controller = DataSelectController(df_train, df_test)    \n","done=False\n","\n","for i in range(iteration):\n","    Controller.reset()#恢复数据集，初始化临时数据存储\n","    \n","    for j in range(episode):\n","        print(\"iteration\",i,\"Episode:\",j)\n","\n","        Controller.train_svc_model()#Step1：训练svc class matching model（合并labeled dataset和pseudo_labeled_dataset）\n","        U_predicted = Controller.predict_unlabeled_set(Controller.unlabeled_train_dataset)\n","        observation, reward, done, info = Controller.step(U_predicted, batch) #\n","        Controller.render()#更新临时数据存储\n","        if(done==True):\n","          done=False\n","          break\n"],"metadata":{"id":"qSkogMhGE0Xr","executionInfo":{"status":"error","timestamp":1663076839999,"user_tz":-540,"elapsed":489982,"user":{"displayName":"yi liu","userId":"07321536629726095967"}},"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1h7HjrzEe_AmhIzK5aHicoKRrGUzmHh4x"},"outputId":"223db021-42bb-421c-9120-246ad2a91a46"},"execution_count":55,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":[],"metadata":{"id":"XUDK51-El19g"}}]}